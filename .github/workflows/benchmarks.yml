name: Benchmarks

on:
  push:
    branches:
      - main
  workflow_dispatch:
    inputs:
      benchmark_filter:
        description: 'Benchmark filter (e.g., *Unit*, *Client*, *Producer*)'
        required: false
        default: '*'

env:
  DOTNET_VERSION: '10.0.x'
  DOTNET_SKIP_FIRST_TIME_EXPERIENCE: true
  DOTNET_CLI_TELEMETRY_OPTOUT: true

jobs:
  unit-benchmarks:
    name: Unit Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: ${{ env.DOTNET_VERSION }}
          dotnet-quality: 'preview'

      - name: Run Unit Benchmarks
        run: |
          dotnet build --configuration Release
          cd tools/Dekaf.Benchmarks
          dotnet run -c Release -- --filter "*Unit*" --exporters json --artifacts ./BenchmarkResults/Unit
        continue-on-error: true

      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-unit
          path: tools/Dekaf.Benchmarks/BenchmarkResults/
          retention-days: 30

  client-benchmarks:
    name: Client Benchmarks (Dekaf vs Confluent)
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: ${{ env.DOTNET_VERSION }}
          dotnet-quality: 'preview'

      - name: Start Kafka
        run: |
          docker run -d --name kafka \
            -p 9092:9092 \
            apache/kafka:latest

          echo "Waiting for Kafka to start..."
          for i in {1..30}; do
            if docker exec kafka /opt/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list 2>/dev/null; then
              echo "Kafka is ready"
              break
            fi
            echo "Attempt $i: Kafka not ready yet..."
            sleep 2
          done

      - name: Run Client Benchmarks
        env:
          KAFKA_BOOTSTRAP_SERVERS: localhost:9092
        run: |
          dotnet build --configuration Release
          cd tools/Dekaf.Benchmarks
          dotnet run -c Release -- --filter "*Client*" --exporters json --artifacts ./BenchmarkResults/Client
        continue-on-error: true

      - name: Stop Kafka
        if: always()
        run: docker rm -f kafka || true

      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-client
          path: tools/Dekaf.Benchmarks/BenchmarkResults/
          retention-days: 30

  benchmark-summary:
    name: Generate Summary
    runs-on: ubuntu-latest
    needs: [unit-benchmarks, client-benchmarks]
    if: always()

    steps:
      - name: Download All Results
        uses: actions/download-artifact@v4
        with:
          pattern: benchmark-results-*
          path: benchmark-results
          merge-multiple: true

      - name: Generate Benchmark Summary
        run: |
          python3 << 'EOF'
          import json
          import os

          def format_time(ns):
              """Format nanoseconds to appropriate unit"""
              if ns is None:
                  return "-"
              if ns >= 1_000_000_000:
                  return f"{ns / 1_000_000_000:.2f} s"
              elif ns >= 1_000_000:
                  return f"{ns / 1_000_000:.2f} ms"
              elif ns >= 1_000:
                  return f"{ns / 1_000:.2f} µs"
              else:
                  return f"{ns:.0f} ns"

          def format_bytes(b):
              """Format bytes to appropriate unit"""
              if b is None or b == 0:
                  return "**0 B** ✓"
              if b >= 1_048_576:
                  return f"{b / 1_048_576:.1f} MB"
              elif b >= 1_024:
                  return f"{b / 1_024:.1f} KB"
              else:
                  return f"{b} B"

          def load_benchmarks(category):
              """Load benchmarks from a category folder"""
              path = f"benchmark-results/{category}/results"
              benchmarks = []
              if os.path.isdir(path):
                  for f in os.listdir(path):
                      if f.endswith('.json'):
                          with open(os.path.join(path, f)) as fp:
                              data = json.load(fp)
                              for b in data.get('Benchmarks', []):
                                  if b.get('Statistics') and b['Statistics'].get('Mean') is not None:
                                      benchmarks.append(b)
              return benchmarks

          def get_ratio(dekaf_time, baseline_time):
              """Calculate performance ratio"""
              if baseline_time and dekaf_time:
                  ratio = dekaf_time / baseline_time
                  return f"{ratio:.2f}x"
              return "-"

          output = []
          output.append("# Benchmark Results\n")

          # Unit Benchmarks
          unit = load_benchmarks("Unit")
          if unit:
              output.append("## Unit Benchmarks (Zero-Allocation Protocol)\n")
              output.append("| Operation | Time | Allocated |")
              output.append("|-----------|------|-----------|")
              for b in sorted(unit, key=lambda x: x.get('MethodTitle', x.get('Method', ''))):
                  name = b.get('MethodTitle', b.get('Method', ''))
                  mean = b['Statistics']['Mean']
                  alloc = b.get('Memory', {}).get('BytesAllocatedPerOperation', 0)
                  output.append(f"| {name} | {format_time(mean)} | {format_bytes(alloc)} |")
              output.append("")

          # Client Benchmarks - Group by operation and show Dekaf vs Confluent
          client = load_benchmarks("Client")
          if client:
              output.append("## Client Benchmarks (Dekaf vs Confluent.Kafka)\n")

              # Group by parameters
              by_params = {}
              for b in client:
                  params = b.get('Parameters', '')
                  by_params.setdefault(params, []).append(b)

              for params in sorted(by_params.keys()):
                  benches = by_params[params]
                  if params:
                      parts = dict(p.split('=') for p in params.split('&') if '=' in p)
                      param_str = ", ".join(f"{k}={v}" for k, v in sorted(parts.items()))
                      output.append(f"### {param_str}\n")

                  output.append("| Operation | Confluent | Dekaf | Ratio | Dekaf Allocated |")
                  output.append("|-----------|-----------|-------|-------|-----------------|")

                  # Match Dekaf and Confluent benchmarks
                  ops = {}
                  for b in benches:
                      method = b.get('Method', '')
                      desc = b.get('MethodTitle', method)
                      if method.startswith('Confluent_'):
                          op_name = desc
                          ops.setdefault(op_name, {})['confluent'] = b
                      elif method.startswith('Dekaf_'):
                          op_name = desc
                          ops.setdefault(op_name, {})['dekaf'] = b

                  for op_name, pair in sorted(ops.items()):
                      conf = pair.get('confluent')
                      dekaf = pair.get('dekaf')

                      conf_time = format_time(conf['Statistics']['Mean']) if conf else "-"
                      dekaf_time = format_time(dekaf['Statistics']['Mean']) if dekaf else "-"

                      ratio = "-"
                      if conf and dekaf:
                          ratio = get_ratio(dekaf['Statistics']['Mean'], conf['Statistics']['Mean'])

                      alloc = dekaf.get('Memory', {}).get('BytesAllocatedPerOperation', 0) if dekaf else 0

                      output.append(f"| {op_name} | {conf_time} | {dekaf_time} | {ratio} | {format_bytes(alloc)} |")

                  output.append("")

          output.append("---\n")
          output.append("*Benchmarks run on GitHub Actions (ubuntu-latest)*\n")
          output.append("*Ratio < 1.0 means Dekaf is faster*")

          # Write to summary
          with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as f:
              f.write('\n'.join(output))
          EOF

  benchmark-history:
    name: Store Benchmark History
    runs-on: ubuntu-latest
    needs: [unit-benchmarks, client-benchmarks]
    if: github.ref == 'refs/heads/main'

    permissions:
      contents: write

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download Benchmark Results
        uses: actions/download-artifact@v4
        with:
          pattern: benchmark-results-*
          path: benchmark-results
          merge-multiple: true

      - name: Combine Benchmark Results
        run: |
          mkdir -p combined-results

          python3 << 'EOF'
          import json
          import os

          combined = {'Benchmarks': []}
          for root, dirs, files in os.walk('benchmark-results'):
              for f in files:
                  if f.endswith('.json') and 'results' in root:
                      path = os.path.join(root, f)
                      print(f"Processing: {path}")
                      with open(path) as fp:
                          data = json.load(fp)
                          for bench in data.get('Benchmarks', []):
                              stats = bench.get('Statistics')
                              if stats is not None and stats.get('Mean') is not None:
                                  combined['Benchmarks'].append(bench)
                              else:
                                  print(f"  Skipping (null Statistics): {bench.get('DisplayInfo', bench.get('Method', 'unknown'))}")

          with open('combined-results/combined.json', 'w') as fp:
              json.dump(combined, fp)

          print(f"\nCombined results: {len(combined['Benchmarks'])} benchmarks")
          EOF

      - name: Store Benchmark Results
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: Dekaf Benchmarks
          tool: 'benchmarkdotnet'
          output-file-path: combined-results/combined.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          alert-threshold: '150%'
          comment-on-alert: true
          fail-on-alert: false
          alert-comment-cc-users: '@thomholloway'
