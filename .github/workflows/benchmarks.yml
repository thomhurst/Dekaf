name: Benchmarks

on:
  push:
    branches:
      - main
  workflow_dispatch:
    inputs:
      benchmark_filter:
        description: 'Benchmark filter (e.g., *Serialization*, *Producer*)'
        required: false
        default: '*Memory*'

env:
  DOTNET_VERSION: '10.0.x'
  DOTNET_SKIP_FIRST_TIME_EXPERIENCE: true
  DOTNET_CLI_TELEMETRY_OPTOUT: true

jobs:
  benchmarks:
    name: Run Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: ${{ env.DOTNET_VERSION }}
          dotnet-quality: 'preview'

      - name: Install GitVersion
        uses: gittools/actions/gitversion/setup@v3
        with:
          versionSpec: '6.x'

      - name: Run Serialization Benchmarks (No Docker Required)
        id: serialization_benchmarks
        run: |
          dotnet build --configuration Release
          cd tools/Dekaf.Benchmarks
          dotnet run -c Release -- --filter "*Serialization*" --exporters json --artifacts ./BenchmarkResults/Serialization
        continue-on-error: true

      - name: Run Memory Benchmarks (No Docker Required)
        id: memory_benchmarks
        run: |
          cd tools/Dekaf.Benchmarks
          dotnet run -c Release -- --filter "*Memory*" --exporters json --artifacts ./BenchmarkResults/Memory
        continue-on-error: true

      - name: Start Kafka
        run: |
          docker run -d --name kafka \
            -p 9092:9092 \
            apache/kafka:latest

          echo "Waiting for Kafka to start..."
          for i in {1..30}; do
            if docker exec kafka /opt/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list 2>/dev/null; then
              echo "Kafka is ready"
              break
            fi
            echo "Attempt $i: Kafka not ready yet..."
            sleep 2
          done

      - name: Run Producer Benchmarks
        id: producer_benchmarks
        env:
          KAFKA_BOOTSTRAP_SERVERS: localhost:9092
        run: |
          cd tools/Dekaf.Benchmarks
          dotnet run -c Release -- --filter "*Producer*" --exporters json --artifacts ./BenchmarkResults/Producer
        continue-on-error: true

      - name: Run Consumer Benchmarks
        id: consumer_benchmarks
        env:
          KAFKA_BOOTSTRAP_SERVERS: localhost:9092
        run: |
          cd tools/Dekaf.Benchmarks
          dotnet run -c Release -- --filter "*Consumer*" --exporters json --artifacts ./BenchmarkResults/Consumer
        continue-on-error: true

      - name: Stop Kafka
        if: always()
        run: docker rm -f kafka || true

      - name: Upload Benchmark Results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: tools/Dekaf.Benchmarks/BenchmarkResults/
          retention-days: 30

      - name: Generate Benchmark Summary
        run: |
          python3 << 'EOF'
          import json
          import os

          def format_time(ns):
              """Format nanoseconds to appropriate unit"""
              if ns is None:
                  return "-"
              if ns >= 1_000_000_000:
                  return f"{ns / 1_000_000_000:.2f} s"
              elif ns >= 1_000_000:
                  return f"{ns / 1_000_000:.2f} ms"
              elif ns >= 1_000:
                  return f"{ns / 1_000:.2f} µs"
              else:
                  return f"{ns:.0f} ns"

          def format_bytes(b):
              """Format bytes to appropriate unit"""
              if b is None or b == 0:
                  return "0 B"
              if b >= 1_048_576:
                  return f"{b / 1_048_576:.1f} MB"
              elif b >= 1_024:
                  return f"{b / 1_024:.1f} KB"
              else:
                  return f"{b} B"

          def load_benchmarks(category):
              """Load benchmarks from a category folder"""
              path = f"tools/Dekaf.Benchmarks/BenchmarkResults/{category}/results"
              benchmarks = []
              if os.path.isdir(path):
                  for f in os.listdir(path):
                      if f.endswith('.json'):
                          with open(os.path.join(path, f)) as fp:
                              data = json.load(fp)
                              for b in data.get('Benchmarks', []):
                                  if b.get('Statistics') and b['Statistics'].get('Mean') is not None:
                                      benchmarks.append(b)
              return benchmarks

          output = []
          output.append("# Benchmark Results\n")

          # Memory Benchmarks - Zero Allocation Focus
          memory = load_benchmarks("Memory")
          if memory:
              output.append("## Memory Benchmarks (Zero-Allocation Protocol)\n")
              output.append("| Operation | Time | Allocated |")
              output.append("|-----------|------|-----------|")
              for b in sorted(memory, key=lambda x: x.get('MethodTitle', x.get('Method', ''))):
                  name = b.get('MethodTitle', b.get('Method', ''))
                  mean = b['Statistics']['Mean']
                  alloc = b.get('Memory', {}).get('BytesAllocatedPerOperation', 0)
                  zero = "**0 B** ✓" if alloc == 0 else format_bytes(alloc)
                  output.append(f"| {name} | {format_time(mean)} | {zero} |")
              output.append("")

          # Serialization Benchmarks - Group by type
          serial = load_benchmarks("Serialization")
          if serial:
              output.append("## Serialization Benchmarks\n")

              # Group by operation type
              dekaf_ops = [b for b in serial if 'Dekaf' in b.get('MethodTitle', b.get('Method', ''))]
              baseline_ops = [b for b in serial if 'Baseline' in b.get('MethodTitle', b.get('Method', ''))]

              if dekaf_ops and baseline_ops:
                  output.append("| Operation | Dekaf | Baseline | Allocated |")
                  output.append("|-----------|-------|----------|-----------|")

                  # Match operations
                  ops = {}
                  for b in dekaf_ops:
                      name = b.get('MethodTitle', b.get('Method', '')).replace('Dekaf Writer: ', '')
                      ops.setdefault(name, {})['dekaf'] = b
                  for b in baseline_ops:
                      name = b.get('MethodTitle', b.get('Method', '')).replace('Baseline Writer: ', '')
                      ops.setdefault(name, {})['baseline'] = b

                  for name, pair in sorted(ops.items()):
                      d = pair.get('dekaf')
                      bl = pair.get('baseline')
                      d_time = format_time(d['Statistics']['Mean']) if d else "-"
                      b_time = format_time(bl['Statistics']['Mean']) if bl else "-"
                      alloc = d.get('Memory', {}).get('BytesAllocatedPerOperation', 0) if d else 0
                      zero = "**0 B** ✓" if alloc == 0 else format_bytes(alloc)
                      output.append(f"| {name} | {d_time} | {b_time} | {zero} |")
              output.append("")

          # Producer Benchmarks - Compare Dekaf vs Confluent
          producer = load_benchmarks("Producer")
          if producer:
              output.append("## Producer Benchmarks (Dekaf vs Confluent.Kafka)\n")

              # Group by test type and parameters
              by_params = {}
              for b in producer:
                  params = b.get('Parameters', '')
                  by_params.setdefault(params, []).append(b)

              for params in sorted(by_params.keys()):
                  benches = by_params[params]
                  if params:
                      # Parse params for header
                      parts = dict(p.split('=') for p in params.split('&') if '=' in p)
                      msg_size = parts.get('MessageSize', '?')
                      batch = parts.get('BatchSize', '?')
                      output.append(f"**Message: {msg_size}B, Batch: {batch}**\n")

                  output.append("| Benchmark | Time | Allocated |")
                  output.append("|-----------|------|-----------|")

                  for b in sorted(benches, key=lambda x: x.get('MethodTitle', '')):
                      name = b.get('MethodTitle', b.get('Method', ''))
                      mean = b['Statistics']['Mean']
                      alloc = b.get('Memory', {}).get('BytesAllocatedPerOperation', 0)
                      zero = "**0 B** ✓" if alloc == 0 else format_bytes(alloc)
                      output.append(f"| {name} | {format_time(mean)} | {zero} |")
                  output.append("")

          # Consumer Benchmarks
          consumer = load_benchmarks("Consumer")
          if consumer:
              output.append("## Consumer Benchmarks (Dekaf vs Confluent.Kafka)\n")

              by_params = {}
              for b in consumer:
                  params = b.get('Parameters', '')
                  by_params.setdefault(params, []).append(b)

              for params in sorted(by_params.keys()):
                  benches = by_params[params]
                  if params:
                      parts = dict(p.split('=') for p in params.split('&') if '=' in p)
                      count = parts.get('MessageCount', '?')
                      size = parts.get('MessageSize', '?')
                      output.append(f"**{count} messages × {size}B**\n")

                  output.append("| Benchmark | Time | Allocated |")
                  output.append("|-----------|------|-----------|")

                  for b in sorted(benches, key=lambda x: x.get('MethodTitle', '')):
                      name = b.get('MethodTitle', b.get('Method', ''))
                      mean = b['Statistics']['Mean']
                      alloc = b.get('Memory', {}).get('BytesAllocatedPerOperation', 0)
                      output.append(f"| {name} | {format_time(mean)} | {format_bytes(alloc)} |")
                  output.append("")

          output.append("---\n")
          output.append("*Benchmarks run on GitHub Actions (ubuntu-latest) against Apache Kafka in Docker*")

          # Write to summary
          with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as f:
              f.write('\n'.join(output))
          EOF

  benchmark-comparison:
    name: Store Benchmark History
    runs-on: ubuntu-latest
    needs: benchmarks
    if: github.ref == 'refs/heads/main'

    permissions:
      contents: write

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download Benchmark Results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results
          path: benchmark-results

      - name: Combine Benchmark Results
        run: |
          # Find all BenchmarkDotNet JSON result files and combine them
          mkdir -p combined-results

          # Create a combined JSON file with all benchmarks
          # Filter out benchmarks with null Statistics (these fail github-action-benchmark)
          python3 << 'EOF'
          import json
          import os

          combined = {'Benchmarks': []}
          for root, dirs, files in os.walk('benchmark-results'):
              for f in files:
                  if f.endswith('.json') and 'results' in root:
                      path = os.path.join(root, f)
                      print(f"Processing: {path}")
                      with open(path) as fp:
                          data = json.load(fp)
                          for bench in data.get('Benchmarks', []):
                              # Only include benchmarks with valid Statistics
                              stats = bench.get('Statistics')
                              if stats is not None and stats.get('Mean') is not None:
                                  combined['Benchmarks'].append(bench)
                              else:
                                  print(f"  Skipping (null Statistics): {bench.get('DisplayInfo', bench.get('Method', 'unknown'))}")

          with open('combined-results/combined.json', 'w') as fp:
              json.dump(combined, fp)

          print(f"\nCombined results: {len(combined['Benchmarks'])} benchmarks")
          EOF

      - name: Store Benchmark Results
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: Dekaf Benchmarks
          tool: 'benchmarkdotnet'
          output-file-path: combined-results/combined.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          alert-threshold: '150%'
          comment-on-alert: true
          fail-on-alert: false
          alert-comment-cc-users: '@thomholloway'
