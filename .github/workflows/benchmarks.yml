name: Benchmarks

on:
  push:
    branches:
      - main
  workflow_dispatch:
    inputs:
      benchmark_filter:
        description: 'Benchmark filter (e.g., *Unit*, *Client*, *Producer*)'
        required: false
        default: '*'

env:
  DOTNET_VERSION: '10.0.x'
  DOTNET_SKIP_FIRST_TIME_EXPERIENCE: true
  DOTNET_CLI_TELEMETRY_OPTOUT: true

jobs:
  unit-benchmarks:
    name: Unit Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: ${{ env.DOTNET_VERSION }}
          dotnet-quality: 'preview'

      - name: Run Unit Benchmarks
        run: |
          dotnet build --configuration Release
          cd tools/Dekaf.Benchmarks
          dotnet run -c Release -- --filter "*Unit*" --exporters json --artifacts ./BenchmarkResults/Unit
        continue-on-error: true

      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-unit
          path: tools/Dekaf.Benchmarks/BenchmarkResults/
          retention-days: 30

  client-benchmarks:
    name: Client Benchmarks (Dekaf vs Confluent)
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: ${{ env.DOTNET_VERSION }}
          dotnet-quality: 'preview'

      - name: Start Kafka
        run: |
          docker run -d --name kafka \
            -p 9092:9092 \
            apache/kafka:latest

          echo "Waiting for Kafka to start..."
          for i in {1..30}; do
            if docker exec kafka /opt/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list 2>/dev/null; then
              echo "Kafka is ready"
              break
            fi
            echo "Attempt $i: Kafka not ready yet..."
            sleep 2
          done

      - name: Run Client Benchmarks
        env:
          KAFKA_BOOTSTRAP_SERVERS: localhost:9092
        run: |
          dotnet build --configuration Release
          cd tools/Dekaf.Benchmarks
          dotnet run -c Release -- --filter "*Client*" --exporters json --artifacts ./BenchmarkResults/Client
        continue-on-error: true

      - name: Stop Kafka
        if: always()
        run: docker rm -f kafka || true

      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-client
          path: tools/Dekaf.Benchmarks/BenchmarkResults/
          retention-days: 30

  benchmark-summary:
    name: Generate Summary
    runs-on: ubuntu-latest
    needs: [unit-benchmarks, client-benchmarks]
    if: always()

    steps:
      - name: Download All Results
        uses: actions/download-artifact@v4
        with:
          pattern: benchmark-results-*
          path: benchmark-results
          merge-multiple: true

      - name: Generate Benchmark Summary
        run: |
          python3 << 'EOF'
          import os
          import glob

          output = []
          output.append("# Dekaf Benchmark Results\n")

          # Add the BenchmarkDotNet markdown reports directly
          # They have proper tables with Ratio columns

          # Producer benchmarks
          producer_md = glob.glob("benchmark-results/Client/results/*ProducerBenchmarks*-github.md")
          if producer_md:
              output.append("## Producer Benchmarks (Dekaf vs Confluent.Kafka)\n")
              output.append("> **Ratio < 1.0 means Dekaf is faster than Confluent**\n")
              with open(producer_md[0]) as f:
                  content = f.read()
                  # Remove the ``` code block wrapper
                  lines = content.split('\n')
                  in_table = False
                  for line in lines:
                      if line.startswith('|'):
                          in_table = True
                      if in_table:
                          output.append(line)
              output.append("")

          # Consumer benchmarks
          consumer_md = glob.glob("benchmark-results/Client/results/*ConsumerBenchmarks*-github.md")
          if consumer_md:
              output.append("## Consumer Benchmarks (Dekaf vs Confluent.Kafka)\n")
              with open(consumer_md[0]) as f:
                  content = f.read()
                  lines = content.split('\n')
                  in_table = False
                  for line in lines:
                      if line.startswith('|'):
                          in_table = True
                      if in_table:
                          output.append(line)
              output.append("")

          # Protocol benchmarks (zero-allocation)
          protocol_md = glob.glob("benchmark-results/Unit/results/*ProtocolBenchmarks*-github.md")
          if protocol_md:
              output.append("## Protocol Benchmarks (Zero-Allocation Wire Protocol)\n")
              output.append("> **Allocated = `-` means zero heap allocations** âœ“\n")
              with open(protocol_md[0]) as f:
                  content = f.read()
                  lines = content.split('\n')
                  in_table = False
                  for line in lines:
                      if line.startswith('|'):
                          in_table = True
                      if in_table:
                          output.append(line)
              output.append("")

          # Serializer benchmarks
          serializer_md = glob.glob("benchmark-results/Unit/results/*SerializerBenchmarks*-github.md")
          if serializer_md:
              output.append("## Serializer Benchmarks\n")
              with open(serializer_md[0]) as f:
                  content = f.read()
                  lines = content.split('\n')
                  in_table = False
                  for line in lines:
                      if line.startswith('|'):
                          in_table = True
                      if in_table:
                          output.append(line)
              output.append("")

          # Compression benchmarks
          compression_md = glob.glob("benchmark-results/Unit/results/*CompressionBenchmarks*-github.md")
          if compression_md:
              output.append("## Compression Benchmarks (Snappy)\n")
              with open(compression_md[0]) as f:
                  content = f.read()
                  lines = content.split('\n')
                  in_table = False
                  for line in lines:
                      if line.startswith('|'):
                          in_table = True
                      if in_table:
                          output.append(line)
              output.append("")

          output.append("---\n")
          output.append("*Benchmarks run on GitHub Actions (ubuntu-latest) with BenchmarkDotNet*")

          # Write to summary
          with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as f:
              f.write('\n'.join(output))
          EOF

  benchmark-history:
    name: Store Benchmark History
    runs-on: ubuntu-latest
    needs: [unit-benchmarks, client-benchmarks]
    if: github.ref == 'refs/heads/main'

    permissions:
      contents: write

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download Benchmark Results
        uses: actions/download-artifact@v4
        with:
          pattern: benchmark-results-*
          path: benchmark-results
          merge-multiple: true

      - name: Combine Benchmark Results
        run: |
          mkdir -p combined-results

          python3 << 'EOF'
          import json
          import os

          combined = {'Benchmarks': []}
          for root, dirs, files in os.walk('benchmark-results'):
              for f in files:
                  if f.endswith('.json') and 'results' in root:
                      path = os.path.join(root, f)
                      print(f"Processing: {path}")
                      with open(path) as fp:
                          data = json.load(fp)
                          for bench in data.get('Benchmarks', []):
                              stats = bench.get('Statistics')
                              if stats is not None and stats.get('Mean') is not None:
                                  combined['Benchmarks'].append(bench)
                              else:
                                  print(f"  Skipping (null Statistics): {bench.get('DisplayInfo', bench.get('Method', 'unknown'))}")

          with open('combined-results/combined.json', 'w') as fp:
              json.dump(combined, fp)

          print(f"\nCombined results: {len(combined['Benchmarks'])} benchmarks")
          EOF

      - name: Store Benchmark Results
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: Dekaf Benchmarks
          tool: 'benchmarkdotnet'
          output-file-path: combined-results/combined.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          alert-threshold: '150%'
          comment-on-alert: true
          fail-on-alert: false
          alert-comment-cc-users: '@thomholloway'
