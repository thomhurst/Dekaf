name: Benchmarks

on:
  push:
    branches:
      - main
  workflow_dispatch:
    inputs:
      benchmark_filter:
        description: 'Benchmark filter (e.g., *Unit*, *Client*, *Producer*)'
        required: false
        default: '*'

env:
  DOTNET_VERSION: '10.0.x'
  DOTNET_SKIP_FIRST_TIME_EXPERIENCE: true
  DOTNET_CLI_TELEMETRY_OPTOUT: true

jobs:
  unit-benchmarks:
    name: Unit Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: ${{ env.DOTNET_VERSION }}
          dotnet-quality: 'preview'

      - name: Run Unit Benchmarks
        run: |
          dotnet build --configuration Release
          cd tools/Dekaf.Benchmarks
          dotnet run -c Release -- --filter "*Unit*" --exporters json --artifacts ./BenchmarkResults/Unit
        continue-on-error: true

      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-unit
          path: tools/Dekaf.Benchmarks/BenchmarkResults/
          retention-days: 30

  client-benchmarks:
    name: Client Benchmarks (Dekaf vs Confluent)
    runs-on: ubuntu-latest
    timeout-minutes: 90

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: ${{ env.DOTNET_VERSION }}
          dotnet-quality: 'preview'

      - name: Start Kafka
        run: |
          docker run -d --name kafka \
            -p 9092:9092 \
            apache/kafka:latest

          echo "Waiting for Kafka to start..."
          for i in {1..30}; do
            if docker exec kafka /opt/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list 2>/dev/null; then
              echo "Kafka is ready"
              break
            fi
            echo "Attempt $i: Kafka not ready yet..."
            sleep 2
          done

      - name: Run Client Benchmarks
        env:
          KAFKA_BOOTSTRAP_SERVERS: localhost:9092
        run: |
          dotnet build --configuration Release
          cd tools/Dekaf.Benchmarks
          dotnet run -c Release -- --filter "*Client*" --exporters json --artifacts ./BenchmarkResults/Client
        continue-on-error: true

      - name: Stop Kafka
        if: always()
        run: docker rm -f kafka || true

      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-client
          path: tools/Dekaf.Benchmarks/BenchmarkResults/
          retention-days: 30

  benchmark-summary:
    name: Generate Summary
    runs-on: ubuntu-latest
    needs: [unit-benchmarks, client-benchmarks]
    if: always()

    steps:
      - name: Download All Results
        uses: actions/download-artifact@v4
        with:
          pattern: benchmark-results-*
          path: benchmark-results
          merge-multiple: true

      - name: Generate Benchmark Summary
        run: |
          python3 << 'EOF'
          import os
          import glob

          output = []
          output.append("# Dekaf Benchmark Results\n")

          # Add the BenchmarkDotNet markdown reports directly
          # They have proper tables with Ratio columns

          # Producer benchmarks
          producer_md = glob.glob("benchmark-results/Client/results/*ProducerBenchmarks*-github.md")
          if producer_md:
              output.append("## Producer Benchmarks (Dekaf vs Confluent.Kafka)\n")
              output.append("> **Ratio < 1.0 means Dekaf is faster than Confluent**\n")
              with open(producer_md[0]) as f:
                  content = f.read()
                  # Remove the ``` code block wrapper
                  lines = content.split('\n')
                  in_table = False
                  for line in lines:
                      if line.startswith('|'):
                          in_table = True
                      if in_table:
                          output.append(line)
              output.append("")

          # Consumer benchmarks
          consumer_md = glob.glob("benchmark-results/Client/results/*ConsumerBenchmarks*-github.md")
          if consumer_md:
              output.append("## Consumer Benchmarks (Dekaf vs Confluent.Kafka)\n")
              with open(consumer_md[0]) as f:
                  content = f.read()
                  lines = content.split('\n')
                  in_table = False
                  for line in lines:
                      if line.startswith('|'):
                          in_table = True
                      if in_table:
                          output.append(line)
              output.append("")

          # Protocol benchmarks (zero-allocation)
          protocol_md = glob.glob("benchmark-results/Unit/results/*ProtocolBenchmarks*-github.md")
          if protocol_md:
              output.append("## Protocol Benchmarks (Zero-Allocation Wire Protocol)\n")
              output.append("> **Allocated = `-` means zero heap allocations** âœ“\n")
              with open(protocol_md[0]) as f:
                  content = f.read()
                  lines = content.split('\n')
                  in_table = False
                  for line in lines:
                      if line.startswith('|'):
                          in_table = True
                      if in_table:
                          output.append(line)
              output.append("")

          # Serializer benchmarks
          serializer_md = glob.glob("benchmark-results/Unit/results/*SerializerBenchmarks*-github.md")
          if serializer_md:
              output.append("## Serializer Benchmarks\n")
              with open(serializer_md[0]) as f:
                  content = f.read()
                  lines = content.split('\n')
                  in_table = False
                  for line in lines:
                      if line.startswith('|'):
                          in_table = True
                      if in_table:
                          output.append(line)
              output.append("")

          # Compression benchmarks
          compression_md = glob.glob("benchmark-results/Unit/results/*CompressionBenchmarks*-github.md")
          if compression_md:
              output.append("## Compression Benchmarks (Snappy)\n")
              with open(compression_md[0]) as f:
                  content = f.read()
                  lines = content.split('\n')
                  in_table = False
                  for line in lines:
                      if line.startswith('|'):
                          in_table = True
                      if in_table:
                          output.append(line)
              output.append("")

          output.append("---\n")
          output.append("*Benchmarks run on GitHub Actions (ubuntu-latest) with BenchmarkDotNet*")

          # Write to summary
          with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as f:
              f.write('\n'.join(output))
          EOF

  benchmark-history:
    name: Store Benchmark History
    runs-on: ubuntu-latest
    needs: [unit-benchmarks, client-benchmarks]
    if: github.ref == 'refs/heads/main'

    permissions:
      contents: write

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download Benchmark Results
        uses: actions/download-artifact@v4
        with:
          pattern: benchmark-results-*
          path: benchmark-results
          merge-multiple: true

      - name: Combine Benchmark Results
        run: |
          mkdir -p combined-results

          python3 << 'EOF'
          import json
          import os

          combined = {'Benchmarks': []}
          for root, dirs, files in os.walk('benchmark-results'):
              for f in files:
                  if f.endswith('.json') and 'results' in root:
                      path = os.path.join(root, f)
                      print(f"Processing: {path}")
                      with open(path) as fp:
                          data = json.load(fp)
                          for bench in data.get('Benchmarks', []):
                              stats = bench.get('Statistics')
                              if stats is not None and stats.get('Mean') is not None:
                                  combined['Benchmarks'].append(bench)
                              else:
                                  print(f"  Skipping (null Statistics): {bench.get('DisplayInfo', bench.get('Method', 'unknown'))}")

          with open('combined-results/combined.json', 'w') as fp:
              json.dump(combined, fp)

          print(f"\nCombined results: {len(combined['Benchmarks'])} benchmarks")
          EOF

      - name: Store Benchmark Results
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: Dekaf Benchmarks
          tool: 'benchmarkdotnet'
          output-file-path: combined-results/combined.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          alert-threshold: '150%'
          comment-on-alert: true
          fail-on-alert: false
          alert-comment-cc-users: '@thomholloway'

  publish-to-docs:
    name: Publish Benchmarks to Documentation
    runs-on: ubuntu-latest
    needs: [unit-benchmarks, client-benchmarks]
    if: github.ref == 'refs/heads/main'

    permissions:
      contents: write

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Download Benchmark Results
        uses: actions/download-artifact@v4
        with:
          pattern: benchmark-results-*
          path: benchmark-results
          merge-multiple: true

      - name: Generate Benchmarks Documentation
        run: |
          python3 << 'EOF'
          import os
          import glob
          from datetime import datetime

          output = []
          output.append("---")
          output.append("sidebar_position: 13")
          output.append("---")
          output.append("")
          output.append("# Benchmark Results")
          output.append("")
          output.append("Live benchmark comparisons between Dekaf and Confluent.Kafka, automatically updated on every commit to main.")
          output.append("")
          output.append(f"**Last Updated:** {datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')}")
          output.append("")
          output.append(":::info")
          output.append("These benchmarks run on GitHub Actions (ubuntu-latest) using BenchmarkDotNet. ")
          output.append("**Ratio < 1.0 means Dekaf is faster than Confluent.Kafka**")
          output.append(":::")
          output.append("")

          # Producer benchmarks
          producer_md = glob.glob("benchmark-results/Client/results/*ProducerBenchmarks*-github.md")
          if producer_md:
              output.append("## Producer Benchmarks")
              output.append("")
              output.append("Comparing Dekaf vs Confluent.Kafka for message production across different scenarios.")
              output.append("")
              with open(producer_md[0]) as f:
                  content = f.read()
                  lines = content.split('\n')
                  in_table = False
                  for line in lines:
                      if line.startswith('|'):
                          in_table = True
                      if in_table:
                          output.append(line)
              output.append("")

          # Consumer benchmarks
          consumer_md = glob.glob("benchmark-results/Client/results/*ConsumerBenchmarks*-github.md")
          if consumer_md:
              output.append("## Consumer Benchmarks")
              output.append("")
              output.append("Comparing Dekaf vs Confluent.Kafka for message consumption.")
              output.append("")
              with open(consumer_md[0]) as f:
                  content = f.read()
                  lines = content.split('\n')
                  in_table = False
                  for line in lines:
                      if line.startswith('|'):
                          in_table = True
                      if in_table:
                          output.append(line)
              output.append("")

          # Protocol benchmarks (zero-allocation)
          protocol_md = glob.glob("benchmark-results/Unit/results/*ProtocolBenchmarks*-github.md")
          if protocol_md:
              output.append("## Protocol Benchmarks")
              output.append("")
              output.append("Zero-allocation wire protocol serialization/deserialization.")
              output.append("")
              output.append(":::tip")
              output.append("**Allocated = `-` means zero heap allocations** - the goal of Dekaf's design!")
              output.append(":::")
              output.append("")
              with open(protocol_md[0]) as f:
                  content = f.read()
                  lines = content.split('\n')
                  in_table = False
                  for line in lines:
                      if line.startswith('|'):
                          in_table = True
                      if in_table:
                          output.append(line)
              output.append("")

          # Serializer benchmarks
          serializer_md = glob.glob("benchmark-results/Unit/results/*SerializerBenchmarks*-github.md")
          if serializer_md:
              output.append("## Serializer Benchmarks")
              output.append("")
              with open(serializer_md[0]) as f:
                  content = f.read()
                  lines = content.split('\n')
                  in_table = False
                  for line in lines:
                      if line.startswith('|'):
                          in_table = True
                      if in_table:
                          output.append(line)
              output.append("")

          # Compression benchmarks
          compression_md = glob.glob("benchmark-results/Unit/results/*CompressionBenchmarks*-github.md")
          if compression_md:
              output.append("## Compression Benchmarks")
              output.append("")
              with open(compression_md[0]) as f:
                  content = f.read()
                  lines = content.split('\n')
                  in_table = False
                  for line in lines:
                      if line.startswith('|'):
                          in_table = True
                      if in_table:
                          output.append(line)
              output.append("")

          output.append("---")
          output.append("")
          output.append("## How to Read These Results")
          output.append("")
          output.append("- **Mean**: Average execution time")
          output.append("- **Error**: Half of 99.9% confidence interval")
          output.append("- **StdDev**: Standard deviation of all measurements")
          output.append("- **Ratio**: Performance relative to baseline (Confluent.Kafka)")
          output.append("  - `< 1.0` = Dekaf is faster")
          output.append("  - `> 1.0` = Confluent is faster")
          output.append("  - `1.0` = Same performance")
          output.append("- **Allocated**: Heap memory allocated per operation")
          output.append("  - `-` = Zero allocations (ideal!)")
          output.append("")
          output.append("*Benchmarks are automatically run on every push to main.*")

          # Write to docs
          os.makedirs('docs/docs', exist_ok=True)
          with open('docs/docs/benchmarks.md', 'w') as f:
              f.write('\n'.join(output))

          print(f"Generated benchmarks.md with {len(output)} lines")
          EOF

      - name: Commit and Push Benchmark Results
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add docs/docs/benchmarks.md
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "docs: Update benchmark results [skip ci]"
            git push || echo "Push failed (likely due to branch protection). Results saved as artifact."
          fi
