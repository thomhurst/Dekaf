name: Stress Tests

on:
  schedule:
    - cron: '0 2 * * 0'  # Weekly Sunday 2 AM UTC
  workflow_dispatch:
    inputs:
      duration_minutes:
        description: 'Test duration in minutes'
        required: false
        default: '15'
      message_size:
        description: 'Message size in bytes'
        required: false
        default: '1000'

concurrency:
  group: stress-tests
  cancel-in-progress: false

jobs:
  # Run each test scenario in parallel
  stress-test:
    strategy:
      fail-fast: false
      matrix:
        include:
          - scenario: producer
            client: dekaf
            name: Dekaf Producer
          - scenario: producer
            client: confluent
            name: Confluent Producer
          - scenario: consumer
            client: dekaf
            name: Dekaf Consumer
          - scenario: consumer
            client: confluent
            name: Confluent Consumer

    name: ${{ matrix.name }}
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout
        uses: actions/checkout@v6

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '10.0.x'

      - name: Build Stress Tests
        run: dotnet build tools/Dekaf.StressTests --configuration Release

      - name: Start Kafka
        run: |
          docker run -d --name kafka \
            -p 9092:9092 \
            -e KAFKA_NODE_ID=1 \
            -e KAFKA_PROCESS_ROLES=broker,controller \
            -e KAFKA_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093 \
            -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 \
            -e KAFKA_CONTROLLER_LISTENER_NAMES=CONTROLLER \
            -e KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT \
            -e KAFKA_CONTROLLER_QUORUM_VOTERS=1@localhost:9093 \
            -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
            -e KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1 \
            -e KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=1 \
            -e KAFKA_LOG_DIRS=/var/lib/kafka/data \
            -e CLUSTER_ID=MkU3OEVBNTcwNTJENDM2Qg \
            -e KAFKA_LOG_RETENTION_MS=10000 \
            -e KAFKA_LOG_RETENTION_BYTES=134217728 \
            -e KAFKA_LOG_SEGMENT_BYTES=33554432 \
            -e KAFKA_LOG_SEGMENT_DELETE_DELAY_MS=100 \
            -e KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS=1000 \
            -e KAFKA_LOG_CLEANUP_POLICY=delete \
            apache/kafka:latest

          echo "Waiting for Kafka to be ready..."
          for i in {1..60}; do
            if docker exec kafka /opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server localhost:9092 >/dev/null 2>&1; then
              echo "Kafka is ready"
              break
            fi
            echo "Waiting... ($i/60)"
            sleep 2
          done

      - name: Run ${{ matrix.name }} Stress Test
        env:
          KAFKA_BOOTSTRAP_SERVERS: localhost:9092
        run: |
          cd tools/Dekaf.StressTests
          dotnet run --configuration Release --no-build -- \
            --duration ${{ github.event.inputs.duration_minutes || '15' }} \
            --message-size ${{ github.event.inputs.message_size || '1000' }} \
            --scenario ${{ matrix.scenario }} \
            --client ${{ matrix.client }} \
            --output ./results

      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: results-${{ matrix.client }}-${{ matrix.scenario }}
          path: tools/Dekaf.StressTests/results/
          retention-days: 90

      - name: Stop Kafka
        if: always()
        run: docker rm -f kafka || true

  # Collect all results and generate summary
  generate-summary:
    needs: stress-test
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Checkout
        uses: actions/checkout@v6

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '10.0.x'

      - name: Download All Results
        uses: actions/download-artifact@v4
        with:
          pattern: results-*
          path: ./all-results
          merge-multiple: true

      - name: List Downloaded Results
        run: |
          echo "=== Downloaded Results ==="
          find ./all-results -type f -name "*.json" | head -20
          echo "=== Contents ==="
          for f in $(find ./all-results -type f -name "*.json"); do
            echo "--- $f ---"
            cat "$f" | head -50
          done

      - name: Merge Results and Generate Summary
        run: |
          python3 << 'EOF'
          import json
          import os
          import glob
          from datetime import datetime

          # Find all JSON result files
          json_files = glob.glob("all-results/**/*.json", recursive=True)
          print(f"Found {len(json_files)} result files")

          if not json_files:
              print("No results found")
              exit(0)

          # Collect all individual results
          all_results = []
          run_started = None
          run_completed = None
          machine_name = None
          processor_count = None

          for json_file in json_files:
              print(f"Processing: {json_file}")
              try:
                  with open(json_file) as f:
                      data = json.load(f)

                  # Handle both single result and wrapped results format
                  if 'results' in data:
                      # Wrapped format from StressTestResults
                      all_results.extend(data['results'])
                      if run_started is None or data.get('runStartedAtUtc', '') < run_started:
                          run_started = data.get('runStartedAtUtc')
                      if run_completed is None or data.get('runCompletedAtUtc', '') > run_completed:
                          run_completed = data.get('runCompletedAtUtc')
                      machine_name = data.get('machineName', machine_name)
                      processor_count = data.get('processorCount', processor_count)
                  else:
                      # Single result format
                      all_results.append(data)

              except Exception as e:
                  print(f"Error processing {json_file}: {e}")

          print(f"Collected {len(all_results)} test results")

          # Create merged results
          merged = {
              'runStartedAtUtc': run_started or datetime.utcnow().isoformat(),
              'runCompletedAtUtc': run_completed or datetime.utcnow().isoformat(),
              'machineName': machine_name or 'GitHub Actions',
              'processorCount': processor_count or 4,
              'results': all_results
          }

          # Save merged results
          os.makedirs('merged-results', exist_ok=True)
          merged_path = f"merged-results/stress-test-results-{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}.json"
          with open(merged_path, 'w') as f:
              json.dump(merged, f, indent=2)
          print(f"Saved merged results to: {merged_path}")

          # Generate markdown summary
          output = []
          output.append("# Dekaf Stress Test Results")
          output.append("")
          output.append(f"**Run Date:** {run_started or 'N/A'}")
          output.append(f"**Machine:** {machine_name or 'GitHub Actions'} ({processor_count or 'N/A'} processors)")
          output.append("")

          # Group results by scenario
          producer_results = [r for r in all_results if 'producer' in r.get('scenario', '').lower()]
          consumer_results = [r for r in all_results if 'consumer' in r.get('scenario', '').lower()]

          def format_throughput_table(results, title):
              if not results:
                  return []

              lines = []
              duration = results[0].get('durationMinutes', 'N/A')
              message_size = results[0].get('messageSizeBytes', 'N/A')

              lines.append(f"## {title} ({duration} minutes, {message_size}B messages)")
              lines.append("")
              lines.append("| Client | Messages/sec | MB/sec | Total Messages | Errors | Ratio |")
              lines.append("|--------|--------------|--------|----------------|--------|-------|")

              # Find baseline (Confluent)
              confluent_rate = 0
              for r in results:
                  if r.get('client', '').lower() == 'confluent':
                      confluent_rate = r.get('throughput', {}).get('averageMessagesPerSecond', 0)
                      break

              if confluent_rate == 0:
                  confluent_rate = min(r.get('throughput', {}).get('averageMessagesPerSecond', 1) for r in results)

              # Sort by throughput descending
              sorted_results = sorted(results, key=lambda r: r.get('throughput', {}).get('averageMessagesPerSecond', 0), reverse=True)

              for r in sorted_results:
                  client = r.get('client', 'Unknown')
                  throughput = r.get('throughput', {})
                  msg_sec = throughput.get('averageMessagesPerSecond', 0)
                  mb_sec = throughput.get('averageMegabytesPerSecond', 0)
                  total = throughput.get('totalMessages', 0)
                  errors = throughput.get('totalErrors', 0)
                  ratio = msg_sec / confluent_rate if confluent_rate > 0 else 1.0

                  lines.append(f"| {client} | {msg_sec:,.0f} | {mb_sec:.2f} | {total:,} | {errors} | {ratio:.2f}x |")

              lines.append("")
              return lines

          def format_gc_table(results):
              if not results:
                  return []

              lines = []
              lines.append("## GC Statistics")
              lines.append("")
              lines.append("| Client | Scenario | Gen0 | Gen1 | Gen2 | Total Allocated |")
              lines.append("|--------|----------|------|------|------|-----------------|")

              for r in sorted(results, key=lambda x: (x.get('client', ''), x.get('scenario', ''))):
                  client = r.get('client', 'Unknown')
                  scenario = r.get('scenario', 'Unknown')
                  gc = r.get('gcStats', {})
                  gen0 = gc.get('gen0Collections', 0)
                  gen1 = gc.get('gen1Collections', 0)
                  gen2 = gc.get('gen2Collections', 0)
                  allocated = gc.get('allocatedBytes', 0)

                  # Format allocated bytes
                  if allocated < 1024:
                      alloc_str = f"{allocated} B"
                  elif allocated < 1024 * 1024:
                      alloc_str = f"{allocated / 1024:.2f} KB"
                  elif allocated < 1024 * 1024 * 1024:
                      alloc_str = f"{allocated / (1024 * 1024):.2f} MB"
                  else:
                      alloc_str = f"{allocated / (1024 * 1024 * 1024):.2f} GB"

                  lines.append(f"| {client} | {scenario} | {gen0} | {gen1} | {gen2} | {alloc_str} |")

              lines.append("")
              return lines

          output.extend(format_throughput_table(producer_results, "Producer Throughput"))
          output.extend(format_throughput_table(consumer_results, "Consumer Throughput"))
          output.extend(format_gc_table(all_results))

          # Write to GitHub summary
          summary_path = os.environ.get('GITHUB_STEP_SUMMARY')
          if summary_path:
              with open(summary_path, 'a') as f:
                  f.write('\n'.join(output))
              print(f"Wrote summary to GITHUB_STEP_SUMMARY")

          # Also print to console
          print('\n'.join(output))
          EOF

      - name: Upload Merged Results
        uses: actions/upload-artifact@v4
        with:
          name: stress-test-results-merged
          path: merged-results/
          retention-days: 90

  publish-to-docs:
    name: Publish Stress Test Results to Documentation
    needs: generate-summary
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' && needs.generate-summary.result == 'success'

    permissions:
      contents: write
      pull-requests: write

    steps:
      - name: Checkout
        uses: actions/checkout@v6

      - name: Download Merged Results
        uses: actions/download-artifact@v4
        with:
          name: stress-test-results-merged
          path: ./results

      - name: Generate Stress Test Documentation
        run: |
          python3 << 'EOF'
          import json
          import os
          import glob
          from datetime import datetime

          output = []
          output.append("---")
          output.append("sidebar_position: 14")
          output.append("---")
          output.append("")
          output.append("# Stress Test Results")
          output.append("")
          output.append("Long-running stress tests comparing sustained performance between Dekaf and Confluent.Kafka under real-world load.")
          output.append("")
          output.append(f"**Last Updated:** {datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')}")
          output.append("")
          output.append(":::info")
          output.append("These tests run weekly (Sunday 2 AM UTC) and can be manually triggered. ")
          output.append("They measure sustained performance over 15+ minutes with real Kafka instances.")
          output.append(":::")
          output.append("")

          # Find and parse JSON results
          json_files = glob.glob("results/**/*.json", recursive=True)

          if json_files:
              # Use the most recent file
              latest_file = sorted(json_files)[-1]
              print(f"Using: {latest_file}")

              try:
                  with open(latest_file) as f:
                      data = json.load(f)

                  results = data.get('results', [])

                  # Group by scenario
                  producer_results = [r for r in results if 'producer' in r.get('scenario', '').lower()]
                  consumer_results = [r for r in results if 'consumer' in r.get('scenario', '').lower()]

                  if producer_results:
                      duration = producer_results[0].get('durationMinutes', 'N/A')
                      message_size = producer_results[0].get('messageSizeBytes', 'N/A')

                      output.append(f"## Producer Performance ({duration} min, {message_size}B)")
                      output.append("")
                      output.append("| Client | Messages/sec | MB/sec | Total | Errors |")
                      output.append("|--------|--------------|--------|-------|--------|")

                      for r in sorted(producer_results, key=lambda x: x.get('throughput', {}).get('averageMessagesPerSecond', 0), reverse=True):
                          t = r.get('throughput', {})
                          output.append(f"| {r.get('client')} | {t.get('averageMessagesPerSecond', 0):,.0f} | {t.get('averageMegabytesPerSecond', 0):.2f} | {t.get('totalMessages', 0):,} | {t.get('totalErrors', 0)} |")

                      output.append("")

                      # Show comparison
                      dekaf = next((r for r in producer_results if r.get('client', '').lower() == 'dekaf'), None)
                      confluent = next((r for r in producer_results if r.get('client', '').lower() == 'confluent'), None)

                      if dekaf and confluent:
                          dekaf_rate = dekaf.get('throughput', {}).get('averageMessagesPerSecond', 0)
                          confluent_rate = confluent.get('throughput', {}).get('averageMessagesPerSecond', 0)
                          if confluent_rate > 0:
                              ratio = dekaf_rate / confluent_rate
                              if ratio > 1.05:
                                  output.append(f":::tip")
                                  output.append(f"**Dekaf is {ratio:.2f}x faster** than Confluent.Kafka for producer throughput!")
                                  output.append(f":::")
                              elif ratio < 0.95:
                                  output.append(f":::note")
                                  output.append(f"Confluent.Kafka is {1/ratio:.2f}x faster for producer throughput.")
                                  output.append(f":::")
                              else:
                                  output.append(f":::note")
                                  output.append(f"Dekaf and Confluent.Kafka have similar producer performance.")
                                  output.append(f":::")
                          output.append("")

                  if consumer_results:
                      duration = consumer_results[0].get('durationMinutes', 'N/A')
                      message_size = consumer_results[0].get('messageSizeBytes', 'N/A')

                      output.append(f"## Consumer Performance ({duration} min, {message_size}B)")
                      output.append("")
                      output.append("| Client | Messages/sec | MB/sec | Total | Errors |")
                      output.append("|--------|--------------|--------|-------|--------|")

                      for r in sorted(consumer_results, key=lambda x: x.get('throughput', {}).get('averageMessagesPerSecond', 0), reverse=True):
                          t = r.get('throughput', {})
                          output.append(f"| {r.get('client')} | {t.get('averageMessagesPerSecond', 0):,.0f} | {t.get('averageMegabytesPerSecond', 0):.2f} | {t.get('totalMessages', 0):,} | {t.get('totalErrors', 0)} |")

                      output.append("")

                  # GC Stats
                  output.append("## Memory & GC Statistics")
                  output.append("")
                  output.append("| Client | Scenario | Gen0 | Gen1 | Gen2 | Total Allocated |")
                  output.append("|--------|----------|------|------|------|-----------------|")

                  for r in sorted(results, key=lambda x: (x.get('client', ''), x.get('scenario', ''))):
                      gc = r.get('gcStats', {})
                      allocated = gc.get('allocatedBytes', 0)
                      if allocated < 1024 * 1024:
                          alloc_str = f"{allocated / 1024:.1f} KB"
                      elif allocated < 1024 * 1024 * 1024:
                          alloc_str = f"{allocated / (1024 * 1024):.1f} MB"
                      else:
                          alloc_str = f"{allocated / (1024 * 1024 * 1024):.2f} GB"

                      output.append(f"| {r.get('client')} | {r.get('scenario')} | {gc.get('gen0Collections', 0)} | {gc.get('gen1Collections', 0)} | {gc.get('gen2Collections', 0)} | {alloc_str} |")

                  output.append("")

              except Exception as e:
                  print(f"Error processing results: {e}")
                  import traceback
                  traceback.print_exc()
          else:
              output.append("## No Results Available")
              output.append("")
              output.append("Stress test results will appear here after the first successful run.")
              output.append("")

          output.append("---")
          output.append("")
          output.append("## About These Tests")
          output.append("")
          output.append("Stress tests measure sustained performance over extended periods:")
          output.append("")
          output.append("- **Real Kafka**: Tests run against actual Apache Kafka instances")
          output.append("- **Parallel Execution**: Each test runs in its own isolated environment")
          output.append("- **Both Clients**: Direct comparison between Dekaf and Confluent.Kafka")
          output.append("- **Memory Monitoring**: Tracks GC behavior and memory usage over time")
          output.append("- **Error Rates**: Ensures stability under load")
          output.append("")

          # Write to docs
          os.makedirs('docs/docs', exist_ok=True)
          with open('docs/docs/stress-tests.md', 'w') as f:
              f.write('\n'.join(output))

          print(f"Generated stress-tests.md with {len(output)} lines")
          EOF

      - name: Create Pull Request
        id: create-pr
        uses: peter-evans/create-pull-request@v7
        with:
          commit-message: 'docs: Update stress test results'
          title: 'docs: Update stress test results'
          body: 'Automated update of stress test results from latest CI run.'
          branch: docs/stress-test-results
          delete-branch: true

      - name: Merge Pull Request
        if: steps.create-pr.outputs.pull-request-number
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: gh pr merge ${{ steps.create-pr.outputs.pull-request-number }} --squash --delete-branch --admin
